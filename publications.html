<!DOCTYPE HTML>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Yiwei Lu - Publications</title>
  <meta name="author" content="Yiwei Lu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
</head>

<body>
  <div class="wrapper">
    <aside class="sidebar">
      <div class="sidebar-content">
        <a href="index.html"><img src="img/Yiwei_headshot.png" alt="profile photo" class="profile-image"></a>
        <div class="contact-info">
          <div class="contact-email">
            <span><i class="fa-solid fa-envelope"></i> Email:</span>
            <a href="mailto:yiwei.lu@uottawa.ca">yiwei.lu@uottawa.ca</a>
          </div>
          <hr style="margin: 12px 0; border: none; border-top: 1px solid #eee;">
          <p><a href="data/yiwei_cv.pdf"><i class="fa-solid fa-file-lines"></i> CV</a></p>
          <p><a href="https://scholar.google.com/citations?hl=en&user=ke0k9PkAAAAJ"><i class="fa-solid fa-book"></i> Google Scholar</a></p>
          <p><a href="https://github.com/yiweilu3"><i class="fa-brands fa-github"></i> Github</a></p>
          <p><a href="https://www.linkedin.com/in/yiwei-lu-b89326183/"><i class="fa-brands fa-linkedin"></i> LinkedIn</a></p>
        </div>
      </div>
    </aside>

    <main class="main-content">
      <header class="main-header">
        <h1>Yiwei Lu</h1>
        <nav class="main-nav">
          <a href="index.html">About</a>
          <a href="publications.html" class="active">Publications</a>
          <a href="students.html">Prospective Students</a>
        </nav>
      </header>

      <section id="publications">
        <h2>Publications</h2>
        <p>(* denotes equal contribution)</p>
        
        <div class="publication-list">
          <article class="publication-item">
            <a href="img/poison.png"><img src="img/poison.png" class="pub-img"></a>
            <div class="pub-details">
              <div class="pub-title">Not All Samples Are Equal: Quantifying Instance-level Difficulty in Targeted Data Poisoning</div>
              <div class="pub-authors">William Xu, <strong>Yiwei Lu</strong>, Yihan Wang, Matthew Y.R. Yang, Zuoqiu Liu, <a href="https://www.gautamkamath.com/">Gautam Kamath</a>, <a href="https://cs.uwaterloo.ca/~y328yu/">Yaoliang Yu</a></div>
              <div class="pub-venue"><a href="https://arxiv.org/abs/2509.06896">arXiv</a></div>
              <p>We quantify instance-level difficulty for targeted data poisoning via ergodic prediction accuracy, poison distance, and poison budget, predicting vulnerability across scenarios.</p>
            </div>
          </article>
          <article class="publication-item">
            <a href="img/memorization.jpg"><img src="img/memorization.jpg" class="pub-img"></a>
            <div class="pub-details">
              <div class="pub-title">Demystifying Foreground-Background Memorization in Diffusion Models</div>
              <div class="pub-authors">Jimmy Z. Di*, <strong>Yiwei Lu*</strong>, <a href="https://cs.uwaterloo.ca/~y328yu/">Yaoliang Yu</a>, <a href="https://www.gautamkamath.com/">Gautam Kamath</a>, <a href="https://adam-dziedzic.com/">Adam Dziedzic</a>, <a href="https://franziska-boenisch.de/">Franziska Boenisch</a></div>
              <div class="pub-venue"><a href="https://arxiv.org/abs/2508.12148">arXiv</a></div>
              <p>We introduce FB-Mem, a segmentation-based metric to classify and quantify memorized regions in images generated by diffusion models, revealing pervasive local memorization and limitations of existing mitigation methods.</p>
            </div>
          </article>

          <article class="publication-item">
            <a href="img/bridgepure.png"><img src="img/bridgepure.png" class="pub-img"></a>
            <div class="pub-details">
              <div class="pub-title">BridgePure: Revealing the Fragility of Black-box Data Protection</div>
              <div class="pub-authors">Yihan Wang*, <strong>Yiwei Lu*</strong>, <a href="http://www.mmrc.iss.ac.cn/~xgao/">Xiaoshan Gao</a>, <a href="https://www.gautamkamath.com/">Gautam Kamath</a>, <a href="https://cs.uwaterloo.ca/~y328yu/">Yaoliang Yu</a></div>
              <div class="pub-venue"><a href="https://arxiv.org/abs/2412.21061">arXiv</a></div>
              <p>We show black-box data protections can be substantially bypassed if a small set of unprotected in-distribution data is available. This small set can be used to train a diffusion bridge model which effectively remove the protection from any previously unseen data within the same distribution.</p>
            </div>
          </article>

          <article class="publication-item">
            <a href="img/MUC.png"><img src="img/MUC.png" class="pub-img"></a>
            <div class="pub-details">
              <div class="pub-title">MUC: Machine Unlearning for Contrastive Learning with Black-box Evaluation</div>
              <div class="pub-authors">Yihan Wang*, <strong>Yiwei Lu*</strong>, <a href="https://gordon-guojun-zhang.github.io/">Guojun Zhang</a>, <a href="https://franziska-boenisch.de/">Franziska Boenisch</a>, <a href="https://adam-dziedzic.com/">Adam Dziedzic</a>, <a href="https://cs.uwaterloo.ca/~y328yu/">Yaoliang Yu</a>, <a href="http://www.mmrc.iss.ac.cn/~xgao/">Xiaoshan Gao</a></div>
              <div class="pub-venue"><em>Transactions on Machine Learning Research (TMLR)</em> / <em>ICML 2024 NextGenAISafety Workshop</em> <font color="#FF8080"><strong>(Oral)</strong></font> / <a href="https://openreview.net/pdf?id=F9pjSDvuM9">paper</a> / <a href="https://arxiv.org/abs/2406.03603">arXiv</a></div>
              <p>We address machine unlearning for contrastive learning pretraining schemes via a novel method called Alignment Calibration. We also propose new auditing tools for data owners to easily validate the effect of unlearning.</p>
            </div>
          </article>
          
          <article class="publication-item">
            <a href="img/MUL.png"><img src="img/MUL.png" class="pub-img"></a>
            <div class="pub-details">
              <div class="pub-title">Machine Unlearning Fails to Remove Data Poisoning Attacks</div>
              <div class="pub-authors"><a href="https://sites.google.com/view/martinpawelczyk/">Martin Pawelczyk*</a>, <a href="https://uwaterloo.ca/scholar/z2di">Jimmy Z. Di*</a>, <strong>Yiwei Lu</strong>, <a href="https://www.gautamkamath.com/">Gautam Kamath</a>, <a href="https://ayush.sekhari.com/">Ayush Sekhari</a>, <a href="https://www.hbs.edu/faculty/Pages/profile.aspx?facId=1326892">Seth Neel</a></div>
              <div class="pub-venue"><em>ICLR 2025 / ICML 2024 Generative AI and Law Workshop</em> <font color="#FF8080"><strong>(Oral)</strong></font> / <a href="https://arxiv.org/abs/2406.17216">arXiv</a></div>
              <p>We find that current approaches for machine unlearning (MUL) are not effective at removing the effect of data poisoning attacks.</p>
            </div>
          </article>

           <article class="publication-item">
            <a href="img/poison_q.png"><img src="img/poison_q.png" class="pub-img"></a>
            <div class="pub-details">
              <div class="pub-title">On the Robustness of Neural Networks Quantization against Data Poisoning Attacks</div>
              <div class="pub-authors"><strong>Yiwei Lu</strong>, Yihan Wang, <a href="https://gordon-guojun-zhang.github.io/">Guojun Zhang</a>, <a href="https://cs.uwaterloo.ca/~y328yu/">Yaoliang Yu</a></div>
              <div class="pub-venue"><em>ICML 2024 NextGenAISafety Workshop</em> / <a href="https://openreview.net/forum?id=YuXWnkhZOj">paper</a></div>
              <p>We find that neural network quantization offers improved robustness against different data poisoning attacks.</p>
            </div>
          </article>

          <article class="publication-item">
            <a href="img/disguise.png"><img src="img/disguise.png" class="pub-img"></a>
            <div class="pub-details">
              <div class="pub-title">Disguised Copyright Infringement of Latent Diffusion Models</div>
              <div class="pub-authors"><strong>Yiwei Lu*</strong>, <a href="https://thesalon.github.io/people/">Matthew Y.R. Yang*</a>, Zuoqiu Liu*, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, <a href="https://cs.uwaterloo.ca/~y328yu/">Yaoliang Yu</a></div>
              <div class="pub-venue"><em>ICML 2024 / ICML 2024 Generative AI and Law Workshop</em> / <a href="https://arxiv.org/abs/2404.06737">arXiv</a></div>
              <p>We reveal the threat of disguised copyright infringement of latent diffusion models, where one constructs a disguise that looks drastically different from the copyrighted sample yet still induces the effect of training Latent Diffusion Models on it.</p>
            </div>
          </article>

          <article class="publication-item">
            <a href="img/satml.png"><img src="img/satml.png" class="pub-img"></a>
            <div class="pub-details">
              <div class="pub-title">Indiscriminate Data Poisoning Attacks on Pre-trained Feature Extractors</div>
              <div class="pub-authors"><strong>Yiwei Lu</strong>, <a href="https://thesalon.github.io/people/">Matthew Y.R. Yang</a>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, <a href="https://cs.uwaterloo.ca/~y328yu/">Yaoliang Yu</a></div>
              <div class="pub-venue"><em>IEEE SaTML 2024</em> / <a href="https://arxiv.org/abs/2402.12626">arXiv</a></div>
              <p>We study indiscriminate data poisoning attacks against pre-trained feature extractors for fine-tuning and transfer learning tasks and propose feature targeted attacks to address optimization difficulty under constraints.</p>
            </div>
          </article>

          <article class="publication-item">
            <a href="img/PC++.jpg"><img src="img/PC++.jpg" class="pub-img"></a>
            <div class="pub-details">
              <div class="pub-title">Understanding Neural Network Binarization with Forward and Backward Proximal Quantizers</div>
              <div class="pub-authors"><strong>Yiwei Lu</strong>, <a href="https://cs.uwaterloo.ca/~y328yu/">Yaoliang Yu</a>, <a href="https://openreview.net/profile?id=~Xinlin_Li2">Xinlin Li</a>, <a href="https://datawisdom.ca/index.htm">Vahid Partovi Nia</a></div>
              <div class="pub-venue"><em>NeurIPS 2023</em> / <a href="https://openreview.net/forum?id=U6fp6IUBdr">paper</a></div>
              <p>We propose forward backward proximal quantizers for understanding approximate gradients in neural network quantization and provide a a new tool for designing new algorithms.</p>
            </div>
          </article>

          <article class="publication-item">
            <a href="img/fmicl.jpg"><img src="img/fmicl.jpg" class="pub-img"></a>
            <div class="pub-details">
              <div class="pub-title">f-MICL: Understanding and Generalizing InfoNCE-based Contrastive Learning</div>
              <div class="pub-authors"><strong>Yiwei Lu*</strong>, <a href="https://gordon-guojun-zhang.github.io/">Guojun Zhang*</a>, <a href="https://ece.uwaterloo.ca/~s232sun/">Sun Sun</a>, <a href="https://uniweb.uottawa.ca/members/4499/profile">Hongyu Guo</a>, <a href="https://cs.uwaterloo.ca/~y328yu/">Yaoliang Yu</a></div>
              <div class="pub-venue"><em>Transactions on Machine Learning Research</em> / <a href="https://openreview.net/forum?id=ZD03VUZmRx&referrer=%5BTMLR%5D(%2Fgroup%3Fid%3DTMLR)">paper</a></div>
              <p>We propose a general and novel loss function on contrastive learning based on f-mutual information. Additionally, we propose a f-Gaussain similarity funcntion with better interpretability and empirical performance.</p>
            </div>
          </article>

          <article class="publication-item">
            <a href="img/cm-gan.jpg"><img src="img/cm-gan.jpg" class="pub-img"></a>
            <div class="pub-details">
              <div class="pub-title">CM-GAN: Stablizing GAN Training with Consistency Models</div>
              <div class="pub-authors"><a href="https://haoyelu.github.io/">Haoye Lu</a>, <strong>Yiwei Lu</strong>, <a href="https://dihjiang.github.io/">Dihong Jiang</a>, <a href="https://scholar.google.com/citations?user=35D6WQsAAAAJ&hl=en">Spencer Ryan Szabados</a>, <a href="https://ece.uwaterloo.ca/~s232sun/">Sun Sun</a>, <a href="https://cs.uwaterloo.ca/~y328yu/">Yaoliang Yu</a></div>
              <div class="pub-venue"><em>ICML 2023 Wrokshop on Structured Probabilistic Inference & Generative Modeling</em> / <a href="https://openreview.net/pdf?id=Uh2WwUyiAv">paper</a></div>
              <p>We propose CM-GAN by combining the main strengths of diffusions and GANs while mitigating their major drawbacks.</p>
            </div>
          </article>

          <article class="publication-item">
            <a href="img/gc.jpg"><img src="img/gc.jpg" class="pub-img"></a>
            <div class="pub-details">
              <div class="pub-title">Exploring the Limits of Model-Targeted Indiscriminate Data Poisoning Attacks</div>
              <div class="pub-authors"><strong>Yiwei Lu</strong>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, <a href="https://cs.uwaterloo.ca/~y328yu/">Yaoliang Yu</a></div>
              <div class="pub-venue"><em>ICML , 2023</em> / <a href="https://openreview.net/pdf?id=r1DAAD9IyE">paper</a></div>
              <p>We find (1) existing indiscriminate attacks are not well-designed (or optimized), and we reduce the performance gap with a new attack; (2) there exists some intrinsic barriers of data poisoning attacks, namely that when the poisoning fraction is smaller than a (easy to calculate) threshold, no attack succeeds.</p>
            </div>
          </article>

          <article class="publication-item">
            <a href="img/poison.png"><img src="img/poison.png" class="pub-img"></a>
            <div class="pub-details">
              <div class="pub-title">Indiscriminate Data Poisoning Attacks on Neural Networks</div>
              <div class="pub-authors"><strong>Yiwei Lu</strong>, <a href="http://www.gautamkamath.com/">Gautam Kamath</a>, <a href="https://cs.uwaterloo.ca/~y328yu/">Yaoliang Yu</a></div>
              <div class="pub-venue"><em>Transactions on Machine Learning Research (also appeared in NeurIPS 2022 ML Safety Workshop and Trustworthy and Socially Responsible Machine Learning (TSRML) Workshop)</em> / <a href="https://openreview.net/pdf?id=x4hmIsWu7e">paper</a> / <a href="https://github.com/watml/TGDA-Attack">code</a></div>
              <p>We find that neural networks are surprisingly hard to (indiscriminate) poison and give better attacks.</p>
            </div>
          </article>

          <article class="publication-item">
            <a href="img/fmicl.jpg"><img src="img/fmicl.jpg" class="pub-img"></a>
            <div class="pub-details">
              <div class="pub-title">f-mutual Information Contrastive Learning</div>
              <div class="pub-authors"><a href="https://gordon-guojun-zhang.github.io/">Guojun Zhang*</a>, <strong>Yiwei Lu*</strong>, <a href="https://ece.uwaterloo.ca/~s232sun/">Sun Sun</a>, <a href="https://uniweb.uottawa.ca/members/4499/profile">Hongyu Guo</a>, <a href="https://cs.uwaterloo.ca/~y328yu/">Yaoliang Yu</a></div>
              <div class="pub-venue"><em>NeurIPS 2021 workshop on self-supervised learning</em> <font color="#FF8080"><strong>(Contributed Talk)</strong></font> / <a href="https://sslneurips21.github.io/files/CameraReady/f-MICL-NeurIPS-workshop.pdf">paper</a> / <a href="https://sslneurips21.github.io/files/Poster/f-micl_poster.pdf">poster</a> / <a href="https://slideslive.com/38972683/fmutual-information-contrastive-learning?ref=speaker-97336-latest">talk</a></div>
              <p>We propose a general and novel loss function on contrastive learning based on f-mutual information.</p>
            </div>
          </article>

          <article class="publication-item">
            <a href="img/adacrowd.jpg"><img src="img/adacrowd.jpg" class="pub-img"></a>
            <div class="pub-details">
              <div class="pub-title">AdaCrowd: Unlabeled Scene Adaptation for Crowd Counting</div>
              <div class="pub-authors"><a href="http://www.cs.umanitoba.ca/~kumarkm/">Mahesh Kumar Krishna Reddy</a>, <a href="http://www.cs.umanitoba.ca/~mrochan/">Mrigank Rochan</a>, <strong>Yiwei Lu</strong>, <a href="http://www.cs.umanitoba.ca/~ywang/">Yang Wang</a></div>
              <div class="pub-venue"><em>IEEE Transactions on Multimedia (TMM)</em>, 2021 / <a href="https://arxiv.org/abs/2010.12141">arXiv</a> / <a href="https://github.com/maheshkkumar/adacrowd">code</a></div>
              <p>We propose a new problem called unlabeled scene adaptive crowd counting.</p>
            </div>
          </article>
          
          <article class="publication-item">
            <a href="img/Intro.jpg"><img src="img/Intro.jpg" class="pub-img"></a>
            <div class="pub-details">
              <div class="pub-title">Few-shot Scene-adaptive Anomaly Detection</div>
              <div class="pub-authors"><strong>Yiwei Lu</strong>, <a href="https://www.facebook.com/yu.frankf">Frank Yu</a>, <a href="http://www.cs.umanitoba.ca/~kumarkm/">Mahesh Kumar Krishna Reddy</a>, <a href="http://www.cs.umanitoba.ca/~ywang/">Yang Wang</a></div>
              <div class="pub-venue"><em>ECCV</em>, 2020 <font color="#FF8080"><strong>(Spotlight)</strong></font> / <a href="https://arxiv.org/abs/2008.03806">arXiv</a> / <a href="https://github.com/yiweilu3/Few-shot-Scene-adaptive-Anomaly-Detection">code</a></div>
              <p>We propose a more realistic problem setting for anomaly detection in surveillance videos and solve it using a meta-learning based algorithm.</p>
            </div>
          </article>

          <article class="publication-item">
            <a href="img/structure.jpg"><img src="img/structure.jpg" class="pub-img"></a>
            <div class="pub-details">
              <div class="pub-title">Structure Learning with Similarity Preserving</div>
              <div class="pub-authors"><a href="https://sites.google.com/site/zhaokanghomepage/">Zhao Kang</a>, <a href="https://en.uestc.edu.cn/">Xiao Lu</a>, <strong>Yiwei Lu</strong>, <a href="https://en.uestc.edu.cn/">Chong Peng</a>, <a href="https://en.uestc.edu.cn/">Wenyu Chen</a>, <a href="https://www.linkedin.com/in/zenglin/">Zenglin Xu</a></div>
              <div class="pub-venue"><em>Neural Networks</em>, 2020 / <a href="https://arxiv.org/pdf/1912.01197">arXiv</a></div>
              <p>We propose a structure learning framework that retains the pairwise similarities between the data points.</p>
            </div>
          </article>

          <article class="publication-item">
            <a href="img/Intro1.jpg"><img src="img/Intro1.jpg" class="pub-img"></a>
            <div class="pub-details">
              <div class="pub-title">Future Frame Prediction Using Convolutional VRNN for Anomaly Detection</div>
              <div class="pub-authors"><strong>Yiwei Lu</strong>, <a href="http://www.cs.umanitoba.ca/~kumarkm/">Mahesh Kumar Krishna Reddy</a>, <a href="https://shahabty.github.io/">Seyed shahabeddin Nabavi</a>, <a href="http://www.cs.umanitoba.ca/~ywang/">Yang Wang</a></div>
              <div class="pub-venue"><em>AVSS</em>, 2019 / <a href="https://arxiv.org/pdf/1909.02168">arXiv</a> / <a href="https://github.com/yiweilu3/CONV-VRNN-for-Anomaly-Detection">code</a></div>
              <p>We propose a novel sequential generative model based on variational autoencoder (VAE) for future frame prediction with convolutional LSTM (ConvLSTM).</p>
            </div>
          </article>

          <article class="publication-item">
            <a href="img/similarity.jpg"><img src="img/similarity.jpg" class="pub-img"></a>
            <div class="pub-details">
              <div class="pub-title">Similarity Learning via Kernel Preserving Embedding</div>
              <div class="pub-authors"><a href="https://sites.google.com/site/zhaokanghomepage/">Zhao Kang</a>, <strong>Yiwei Lu</strong>, <a href="https://en.uestc.edu.cn/">Yuanzhang Su</a>, <a href="https://en.uestc.edu.cn/">Changsheng Li</a>, <a href="https://www.linkedin.com/in/zenglin/">Zenglin Xu</a></div>
              <div class="pub-venue"><em>AAAI</em>, 2019 / <a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/4301/4179">PDF</a></div>
              <p>We propose a novel similarity learning framework by minimizing the reconstruction error of kernel matrices, rather than the reconstruction error of original data adopted by existing work.</p>
            </div>
          </article>

          <article class="publication-item">
            <a href="img/ss.jpg"><img src="img/ss.jpg" class="pub-img"></a>
            <div class="pub-details">
              <div class="pub-title">Semantic Segmentation in Compressed Videos</div>
              <div class="pub-authors"><strong>Yiwei Lu*</strong>, <a href="http://www.cs.umanitoba.ca">Ang Li*</a>, <a href="http://www.cs.umanitoba.ca/~ywang/">Yang Wang</a></div>
              <div class="pub-venue"><em>MMSP</em>, 2019 / <a href="https://www.cs.umanitoba.ca/~ywang/papers/mmsp19_compress.pdf8">PDF</a></div>
              <p>We propose a ConvLSTM-based model to perform semantic segmentation on compressed videos directly. This significantly speed up the training and test speed.</p>
            </div>
          </article>
        </div>

        <div id="thesis">
          <h2>Thesis</h2>
          <p><a href="https://uwspace.uwaterloo.ca/items/af058869-53db-4812-be93-cb14afcdbc65">Trustworthy Machine Learning with Data in the Wild</a> - Yiwei Lu, Ph.D. thesis, Cheriton School of Computer Science, University of Waterloo, 2025.</p>
          <p><a href="https://mspace.lib.umanitoba.ca/xmlui/handle/1993/34793">Anomaly Detection in Surveillance Videos using Deep Learning</a> - Yiwei Lu, M.Sc. thesis, Department of Computer Science, University of Manitoba, June 2020.</p>
        </div>
      </section>

      <footer>
        <p>Last updated: <script>document.write(document.lastModified);</script></p>
      </footer>
    </main>
  </div>
  <button class="theme-toggle-btn" id="theme-toggle" title="Toggle dark/light mode">üåô</button>
  <script>
    const toggleBtn = document.getElementById('theme-toggle');
    const body = document.body;
    if (localStorage.getItem('theme') === 'dark') {
      body.classList.add('dark-mode');
      toggleBtn.textContent = '‚òÄÔ∏è';
    }
    toggleBtn.addEventListener('click', () => {
      body.classList.toggle('dark-mode');
      const isDark = body.classList.contains('dark-mode');
      toggleBtn.textContent = isDark ? '‚òÄÔ∏è' : 'üåô';
      localStorage.setItem('theme', isDark ? 'dark' : 'light');
    });
  </script>
</body>
</html> 